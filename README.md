# Multiparty computation of a Finite State Machine
Built upon https://github.com/data61/MP-SPDZ following https://www.researchgate.net/publication/332877290_Multiparty_Evaluation_of_Finite_State_Machine

## Docker
### Generate Docker Image
`docker build -t mp-fsm-eval .`

Uses a lot of resources (15m+ of computation and 15GB+ of RAM) on wsl2 (not tested on linux).

On windows wsl2 seems to not free RAM when using `RUN make -j 8` command.
To free said memory close Docker and run `wsl --shutdown`.

### Container Instructions
`docker run -it --name mp-fsm-eval mp-fsm-eval`

Runs the container in PseudoTerminal mode, needed since ENTRYPOINT is `bash`.

## Multiparty FSM evaluation
### Preprocess Private Data
Private data has to be encoded as an element of GF(2^n) with appropiate n (default is 128).
To obtain said data first encode it as a hexadecimal string ("7fa01") then process it with `./gen_input_f2n.x`
and save the result in the appropriate file `Player-Data/Private-Input-{PLAYER_ID}`.

Example Pipeline:
- `echo 5 ff a1c 1 1 2d1c0 > gf2n_vals.in && ./gen_input_f2n.x && mv gf2n_vals.out Player-Data/Private-Input-0`

Note that the first input (5) is the number of hexadecimal strings to convert.

### To Run the FSM evaluation 
To run the FSM evaluation first compile `fsm_eval` following the template>

`./compile.py -M fsm_eval N_STATES N_SYMBOLS INPUT_SIZE_P0 INPUT_SIZE_P1 [INPUT_SIZE_P2 ...]`

Then connect the various players running each following:

`./spdz2k-party.x PLAYER_ID fsm_eval-{N_STATES}-{N_SYMBOLS}-{INPUT_SIZE_P0}-{...} -N N_PLAYERS -h HOSTNAME_OF_PLAYER_0 -pn BASE_PORT_NUMBER`

Base port number is required when hostname is not the default option (localhost).

### Tests
#### Same host (example computation)
A test case printing various details of computation is available as `test_fsm1` to execute it run:

`./compile.py -M test_fsm1`

`./Scripts/spdz2k.sh test_fsm1`

The last script starts 2 `spdz2k-party.x` virtual machines on the same host.

#### Costumized Test
Added Notebook to generate Testing Suites based on a .json scheme.
See [Tests](./Tests) for examples on usage.

Notebook is divided in 2 sections: 
- Script Generation
  - Given a .json generate all instructions to start containers, execute test and terminate.
- Parsing
  - Given an output file generated by running the previous script with output redirection `> OUTFILE 2>&1` generates a Pandas Dataset and `.cvs`.
